{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMcMBL5r8JRU7Rb6RkgQN/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/hwr/blob/master/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNcGj5mhxUS3",
        "colab_type": "text"
      },
      "source": [
        "# Simple HWR\n",
        "\n",
        "Implementation of Gated Convolutional Recurrent Neural Network for Handwriting Recognition as recorded in [Bluche](http://ieeexplore.ieee.org/document/8270042/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9G86zuMzPpX",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb0qJGHWy5ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import time\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import pdb\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"Request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZinQgsSyzAGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"drive/My Drive/datasets/iam.zip\" \"/content\"\n",
        "!unzip -q iam.zip\n",
        "!rm iam.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c2gUImtzX5a",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yax5F_YZyvii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IamDataset(Dataset):\n",
        "  def __init__(self, desired_size=(128, 512)):\n",
        "    if not os.path.exists('/content/labels.csv'):\n",
        "      raise Exception('Iam dataset does not exist in /content/labels.csv')\n",
        "\n",
        "    self.desired_size = desired_size\n",
        "    self.path = '/content/images/'\n",
        "    self.df = pd.read_csv('/content/labels.csv', sep='\\t', header=None, names=['word', 'seg', 'transcription'])\n",
        "    self.df = self.df.drop(['seg'], axis=1)\n",
        "    self.df = self.df.drop(self.df[self.df['transcription'] == '.'].index)\n",
        "    self.df = self.df.drop(self.df[self.df['transcription'] == '!'].index)\n",
        "    self.df = self.df.drop(self.df[self.df['transcription'] == ','].index)\n",
        "    self.df = self.df.drop(self.df[self.df['transcription'] == ';'].index)\n",
        "    self.df = self.df.drop(self.df[self.df['transcription'] == ')'].index)\n",
        "    self.df = self.df.drop(self.df[self.df['transcription'] == '('].index)\n",
        "    self.df = self.df.reset_index()\n",
        "    \n",
        "  def get_df(self):\n",
        "    return self.df\n",
        "\n",
        "  def tensor_image(self, path):\n",
        "    img = Image.open(path + '.png')\n",
        "    img = self.resize(img)\n",
        "    x = transforms.functional.to_tensor(img)\n",
        "\n",
        "    # Look into automatically resizing or adding padding to images\n",
        "    # With a GAN, we will likely need all the input images to be the same size\n",
        "\n",
        "    return x\n",
        "\n",
        "  def resize(self, img):\n",
        "    img_size = np.array(img).shape\n",
        "\n",
        "    img_ratio = img_size[0] / img_size[1]\n",
        "    desired_ratio = self.desired_size[0] / self.desired_size[1]\n",
        "\n",
        "    if img_ratio >= desired_ratio:\n",
        "      # Solve by height\n",
        "      new_height = self.desired_size[0]\n",
        "      new_width = int(self.desired_size[0] // img_ratio)\n",
        "    else:\n",
        "      new_height = int(self.desired_size[1] * img_ratio)\n",
        "      new_width = self.desired_size[1]\n",
        "      # Solve by width\n",
        "\n",
        "    img = np.array(img.resize((new_width, new_height)))\n",
        "\n",
        "    border_top = self.desired_size[0] - new_height\n",
        "    border_right = self.desired_size[1] - new_width\n",
        "\n",
        "    border_img = cv2.copyMakeBorder(\n",
        "        img,\n",
        "        top=border_top,\n",
        "        bottom=0,\n",
        "        left=0,\n",
        "        right=border_right,\n",
        "        borderType=cv2.BORDER_CONSTANT,\n",
        "        value=[255]\n",
        "    )\n",
        "\n",
        "    return border_img\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = self.tensor_image('images/' + self.df['word'][index])\n",
        "\n",
        "    return img, self.df['transcription'][index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDWlD4u6zSJM",
        "colab_type": "text"
      },
      "source": [
        "### Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYNnBz-yzGgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, dim=128, layers=4):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "    self.dim = dim\n",
        "\n",
        "    # input_size, hidden_size, num_layers, bidirectional=True\n",
        "    self.gru = nn.GRU(1, dim, layers, bidirectional=True, batch_first=True)\n",
        "    self.fc = nn.Linear(256, 16) # used to match up shapes when given to generator\n",
        "\n",
        "    self.hidden_init_state = torch.randn((8, 1, dim), requires_grad=True)\n",
        "    self.embedding = nn.Embedding(128, 1)\n",
        "\n",
        "  def getRepresentation(words):\n",
        "    charlists = []\n",
        "    zeros = np.zeros(20)\n",
        "\n",
        "    if type(words) == str:\n",
        "      charlist = [ord(c) for c in words]\n",
        "      charlist = np.concatenate((charlist, zeros))\n",
        "      charlists.append(charlist[:16])\n",
        "\n",
        "      return torch.tensor(charlists).long().cuda()\n",
        "\n",
        "    for word in words:\n",
        "      charlist = [ord(c) for c in word]\n",
        "      charlist = np.concatenate((charlist, zeros))\n",
        "      charlists.append(charlist[:16])\n",
        "\n",
        "    return torch.tensor(charlists).long().unsqueeze(2).cuda()\n",
        "\n",
        "  # input => (tuple of strings)\n",
        "  def getEmbedding(words, embedding):\n",
        "    rep = Encoder.getRepresentation(words)\n",
        "    \n",
        "    return embedding(rep)\n",
        "\n",
        "  def init_hidden(self, batch):\n",
        "    return torch.zeros(self.layers * 2, batch, self.dim).cuda()\n",
        "\n",
        "  def weight_init(self, mean, std):\n",
        "      for m in self._modules:\n",
        "          normal_init(self._modules[m], mean, std)\n",
        "    \n",
        "  # input -> embedding\n",
        "  def forward(self, embedding):\n",
        "    hidden_state = self.init_hidden(embedding.shape[0])\n",
        "\n",
        "    out, _ = self.gru(embedding.float(), hidden_state)\n",
        "    out = self.fc(out)\n",
        "    out = out.view(-1, 1, 1, 256)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2-N6mmjytK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Recognizer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Recognizer, self).__init__()\n",
        "\n",
        "    self.hidden_size = 256\n",
        "    self.num_layers = 1\n",
        "    self.max_length = 20\n",
        "\n",
        "    # Encoder\n",
        "    self.conv1 = nn.Conv2d(1, 8, 3, 1, 1)\n",
        "    self.tanh1 = nn.Tanh()\n",
        "    self.conv2 = nn.Conv2d(8, 16, 3, 1, 1) # 4x2\n",
        "    self.tanh2 = nn.Tanh()\n",
        "    self.conv3 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "    self.tanh3 = nn.Tanh()\n",
        "    self.conv4 = nn.Conv2d(32, 64, 3, 1, 1) # 4x2\n",
        "    self.tanh4 = nn.Tanh()\n",
        "    self.conv5 = nn.Conv2d(64, 128, 3, 1, 1)\n",
        "    self.tanh5 = nn.Tanh()\n",
        "\n",
        "    self.gate1 = nn.Sequential(nn.Conv2d(16, 16, 3, 1, 1), nn.Sigmoid()) \n",
        "    self.gate2 = nn.Sequential(nn.Conv2d(32, 32, 3, 1, 1), nn.Sigmoid())\n",
        "    self.gate3 = nn.Sequential(nn.Conv2d(64, 64, 3, 1, 1), nn.Sigmoid())\n",
        "\n",
        "    # Maxpool\n",
        "    self.mp = nn.MaxPool2d((32, 1))\n",
        "    # reshape out.view(-1, 512, 1)\n",
        "\n",
        "    # Decoder\n",
        "    self.gru1 = nn.GRU(128, self.hidden_size, self.num_layers, bidirectional=True, batch_first=True)\n",
        "    self.fc1 = nn.Linear(self.hidden_size * 2, 128)\n",
        "    self.gru2 = nn.GRU(128, self.hidden_size, self.num_layers, bidirectional=True, batch_first=True)\n",
        "    self.fc2 = nn.Linear(self.hidden_size * 2, 16)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return torch.zeros(self.num_layers * 2, batch_size, self.hidden_size)\n",
        "\n",
        "  def weight_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        normal_init(self._modules[m], mean, std)\n",
        "\n",
        "  # Input -> (Batch, 1, 128, 256)\n",
        "  def forward(self, x):\n",
        "    # Encoder\n",
        "    out = self.conv1(x)\n",
        "    out = self.tanh1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.tanh2(out)\n",
        "\n",
        "    gate1 = self.gate1(out)\n",
        "    out = out * gate1\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.tanh3(out)\n",
        "\n",
        "    gate2 = self.gate2(out)\n",
        "    out = out * gate2\n",
        "\n",
        "    out = self.conv4(out)\n",
        "    out = self.tanh4(out)\n",
        "\n",
        "    gate3 = self.gate3(out)\n",
        "    out = out * gate3\n",
        "\n",
        "    out = self.conv5(out)\n",
        "    out = self.tanh5(out)\n",
        "\n",
        "    # Max Pooling across vertical dimension\n",
        "    out = self.mp(out)\n",
        "\n",
        "    # Decoder\n",
        "    out = out.view(-1, 128, 128)\n",
        "\n",
        "    out, _ = self.gru1(out)\n",
        "    out = self.fc1(out)\n",
        "    out, _ = self.gru2(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.softmax(out)\n",
        "\n",
        "    # Change shape so that we can pass directly to CTC-Loss\n",
        "    out = out.permute(2, 0, 1)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GbeMhxKzcyC",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgU8u-EtzihY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "8867e26d-4570-4215-b187-4a04de070ae1"
      },
      "source": [
        "# In future, potentially apply gradient balancing\n",
        "\n",
        "def train():\n",
        "  try:\n",
        "    EPOCHS = 200\n",
        "    BATCH_SIZE = 250\n",
        "    VALIDATION_EVERY\n",
        "\n",
        "    dataset = IamDataset(desired_size=(32, 128))\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(.8 * len(dataset)) + 1, int(.2 * len(dataset))])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "    recognizer = Recognizer().cuda()\n",
        "    optimizer = optim.Adam(recognizer.parameters(), lr=4e-4, betas=(0, .999))\n",
        "    objective = nn.CTCLoss(blank=0, reduction='none', zero_infinity=True)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "      loop = tqdm(total=len(data_loader), position=0, leave=False)\n",
        "\n",
        "      for batch_num, (real_imgs, words) in enumerate(data_loader):\n",
        "        batch_size = real_imgs.shape[0]\n",
        "\n",
        "        real_imgs, labels = real_imgs.cuda(), Encoder.getRepresentation(words).cuda()\n",
        "\n",
        "        rec_optimizer.zero_grad()\n",
        "\n",
        "        real_word_labels = labels.squeeze().long()\n",
        "        input_lengths = torch.full((batch_size,), 16, dtype=torch.long) # (BATCH_SIZE) -> Sequence_Length\n",
        "        target_lengths = word_lengths_tensor(words) # (BATCH_SIZE) -> Word lengths\n",
        "\n",
        "        real_preds = recognizer(real_imgs)\n",
        "\n",
        "        loss = rec_objective(real_preds, real_word_labels, input_lengths, target_lengths)\n",
        "        loss = torch.mean(rec_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(rec_loss.item())\n",
        "\n",
        "        if batch_num % VALIDATION_EVERY == 0:\n",
        "          # Same code but for validation set\n",
        "\n",
        "        loop.set_description('Epoch: {}, Generator Loss: {:.4f}, Discriminator Loss: {:.4f}, Recognizer Loss: {:.4f}, AvgGenerator Loss: {:.4f}, AvgDiscriminator Loss: {:.4f}, AvgRecognizer Loss: {:.4f}'.format(\n",
        "            epoch, gen_loss.item(), disc_loss.item(), rec_loss.item(), np.mean(g_losses), np.mean(d_losses), np.mean(r_losses)))\n",
        "        loop.update(1)\n",
        "\n",
        "      loop.close()\n",
        "    \n",
        "  except:\n",
        "    __ITB__()\n",
        "    gc.collect()\n",
        "  finally:\n",
        "    return generator, recognizer, discriminator, g_losses, d_losses, r_losses"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-253487878e4c>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    loop.set_description('Epoch: {}, Generator Loss: {:.4f}, Discriminator Loss: {:.4f}, Recognizer Loss: {:.4f}, AvgGenerator Loss: {:.4f}, AvgDiscriminator Loss: {:.4f}, AvgRecognizer Loss: {:.4f}'.format(\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0tviNtNzgVu",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    }
  ]
}